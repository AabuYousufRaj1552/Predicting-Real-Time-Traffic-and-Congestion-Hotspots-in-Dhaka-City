{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T17:26:43.746414Z",
     "iopub.status.busy": "2025-01-07T17:26:43.746115Z",
     "iopub.status.idle": "2025-01-07T17:29:18.802526Z",
     "shell.execute_reply": "2025-01-07T17:29:18.801767Z",
     "shell.execute_reply.started": "2025-01-07T17:26:43.746390Z"
    },
    "id": "RuUsgDgE1C8Z",
    "outputId": "ecf1a38f-3c1f-41d0-8079-e4fa79bbb7cb",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Classification report for CNN with image size 128:\n",
      "                  precision    recall  f1-score     support\n",
      "heavy traffic      0.220238  0.909836  0.354633  122.000000\n",
      "light traffic      0.187500  0.027027  0.047244  111.000000\n",
      "moderate traffic   0.000000  0.000000  0.000000  131.000000\n",
      "no traffic         0.522727  0.114428  0.187755  201.000000\n",
      "accuracy           0.242478  0.242478  0.242478    0.242478\n",
      "macro avg          0.232616  0.262823  0.147408  565.000000\n",
      "weighted avg       0.270354  0.242478  0.152651  565.000000\n",
      "\n",
      "==================================================\n",
      "\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Classification report for CNN with image size 256:\n",
      "                  precision    recall  f1-score     support\n",
      "heavy traffic      0.204301  0.155738  0.176744  122.000000\n",
      "light traffic      0.219048  0.207207  0.212963  111.000000\n",
      "moderate traffic   0.194444  0.213740  0.203636  131.000000\n",
      "no traffic         0.363229  0.402985  0.382075  201.000000\n",
      "accuracy           0.267257  0.267257  0.267257    0.267257\n",
      "macro avg          0.245255  0.244918  0.243855  565.000000\n",
      "weighted avg       0.261452  0.267257  0.263142  565.000000\n",
      "\n",
      "==================================================\n",
      "\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step\n",
      "Classification report for CNN with image size 512:\n",
      "                  precision    recall  f1-score    support\n",
      "heavy traffic      0.179153  0.450820  0.256410  122.00000\n",
      "light traffic      0.236111  0.153153  0.185792  111.00000\n",
      "moderate traffic   0.138889  0.038168  0.059880  131.00000\n",
      "no traffic         0.233333  0.174129  0.199430  201.00000\n",
      "accuracy           0.198230  0.198230  0.198230    0.19823\n",
      "macro avg          0.196872  0.204068  0.175378  565.00000\n",
      "weighted avg       0.200282  0.198230  0.176699  565.00000\n",
      "\n",
      "==================================================\n",
      "\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 170ms/step\n",
      "Classification report for ResNet50 with image size 128:\n",
      "                  precision    recall  f1-score     support\n",
      "heavy traffic      1.000000  0.000000  0.000000  122.000000\n",
      "light traffic      1.000000  0.000000  0.000000  111.000000\n",
      "moderate traffic   1.000000  0.000000  0.000000  131.000000\n",
      "no traffic         0.355752  1.000000  0.524804  201.000000\n",
      "accuracy           0.355752  0.355752  0.355752    0.355752\n",
      "macro avg          0.838938  0.250000  0.131201  565.000000\n",
      "weighted avg       0.770807  0.355752  0.186700  565.000000\n",
      "\n",
      "==================================================\n",
      "\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 247ms/step\n",
      "Classification report for ResNet50 with image size 256:\n",
      "                  precision    recall  f1-score     support\n",
      "heavy traffic      1.000000  0.000000  0.000000  122.000000\n",
      "light traffic      1.000000  0.000000  0.000000  111.000000\n",
      "moderate traffic   1.000000  0.000000  0.000000  131.000000\n",
      "no traffic         0.355752  1.000000  0.524804  201.000000\n",
      "accuracy           0.355752  0.355752  0.355752    0.355752\n",
      "macro avg          0.838938  0.250000  0.131201  565.000000\n",
      "weighted avg       0.770807  0.355752  0.186700  565.000000\n",
      "\n",
      "==================================================\n",
      "\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 320ms/step\n",
      "Classification report for ResNet50 with image size 512:\n",
      "                  precision    recall  f1-score     support\n",
      "heavy traffic      1.000000  0.000000  0.000000  122.000000\n",
      "light traffic      0.186747  0.279279  0.223827  111.000000\n",
      "moderate traffic   0.221918  0.618321  0.326613  131.000000\n",
      "no traffic         0.352941  0.059701  0.102128  201.000000\n",
      "accuracy           0.219469  0.219469  0.219469    0.219469\n",
      "macro avg          0.440401  0.239325  0.163142  565.000000\n",
      "weighted avg       0.429631  0.219469  0.156033  565.000000\n",
      "\n",
      "==================================================\n",
      "\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 125ms/step\n",
      "Classification report for MobileNetV2 with image size 128:\n",
      "                  precision    recall  f1-score     support\n",
      "heavy traffic      0.304348  0.114754  0.166667  122.000000\n",
      "light traffic      0.107692  0.063063  0.079545  111.000000\n",
      "moderate traffic   0.218487  0.595420  0.319672  131.000000\n",
      "no traffic         0.453608  0.218905  0.295302  201.000000\n",
      "accuracy           0.253097  0.253097  0.253097    0.253097\n",
      "macro avg          0.271034  0.248036  0.215297  565.000000\n",
      "weighted avg       0.298905  0.253097  0.230789  565.000000\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-64998b46e53e>:25: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  model = MobileNetV2(input_shape=(size, size, 3), include_top=False, weights='imagenet')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 150ms/step\n",
      "Classification report for MobileNetV2 with image size 256:\n",
      "                  precision    recall  f1-score     support\n",
      "heavy traffic      0.400000  0.081967  0.136054  122.000000\n",
      "light traffic      0.207547  0.297297  0.244444  111.000000\n",
      "moderate traffic   0.240678  0.541985  0.333333  131.000000\n",
      "no traffic         0.418605  0.179104  0.250871  201.000000\n",
      "accuracy           0.265487  0.265487  0.265487    0.265487\n",
      "macro avg          0.316707  0.275088  0.241176  565.000000\n",
      "weighted avg       0.331869  0.265487  0.243936  565.000000\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-64998b46e53e>:25: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  model = MobileNetV2(input_shape=(size, size, 3), include_top=False, weights='imagenet')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 196ms/step\n",
      "Classification report for MobileNetV2 with image size 512:\n",
      "                  precision    recall  f1-score     support\n",
      "heavy traffic      0.227488  0.786885  0.352941  122.000000\n",
      "light traffic      0.000000  0.000000  0.000000  111.000000\n",
      "moderate traffic   0.272727  0.022901  0.042254  131.000000\n",
      "no traffic         0.423077  0.273632  0.332326  201.000000\n",
      "accuracy           0.272566  0.272566  0.272566    0.272566\n",
      "macro avg          0.230823  0.270854  0.181880  565.000000\n",
      "weighted avg       0.262866  0.272566  0.204233  565.000000\n",
      "\n",
      "==================================================\n",
      "\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 240ms/step\n",
      "Classification report for EfficientNetB0 with image size 128:\n",
      "                  precision   recall  f1-score    support\n",
      "heavy traffic      1.000000  0.00000  0.000000  122.00000\n",
      "light traffic      0.196460  1.00000  0.328402  111.00000\n",
      "moderate traffic   1.000000  0.00000  0.000000  131.00000\n",
      "no traffic         1.000000  0.00000  0.000000  201.00000\n",
      "accuracy           0.196460  0.19646  0.196460    0.19646\n",
      "macro avg          0.799115  0.25000  0.082101  565.00000\n",
      "weighted avg       0.842136  0.19646  0.064518  565.00000\n",
      "\n",
      "==================================================\n",
      "\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 266ms/step\n",
      "Classification report for EfficientNetB0 with image size 256:\n",
      "                  precision   recall  f1-score    support\n",
      "heavy traffic      1.000000  0.00000  0.000000  122.00000\n",
      "light traffic      0.196460  1.00000  0.328402  111.00000\n",
      "moderate traffic   1.000000  0.00000  0.000000  131.00000\n",
      "no traffic         1.000000  0.00000  0.000000  201.00000\n",
      "accuracy           0.196460  0.19646  0.196460    0.19646\n",
      "macro avg          0.799115  0.25000  0.082101  565.00000\n",
      "weighted avg       0.842136  0.19646  0.064518  565.00000\n",
      "\n",
      "==================================================\n",
      "\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 336ms/step\n",
      "Classification report for EfficientNetB0 with image size 512:\n",
      "                  precision    recall  f1-score     support\n",
      "heavy traffic      1.000000  0.000000  0.000000  122.000000\n",
      "light traffic      1.000000  0.000000  0.000000  111.000000\n",
      "moderate traffic   1.000000  0.000000  0.000000  131.000000\n",
      "no traffic         0.355752  1.000000  0.524804  201.000000\n",
      "accuracy           0.355752  0.355752  0.355752    0.355752\n",
      "macro avg          0.838938  0.250000  0.131201  565.000000\n",
      "weighted avg       0.770807  0.355752  0.186700  565.000000\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import MobileNetV2, ResNet50, EfficientNetB0\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Define models and image sizes\n",
    "models = [\"CNN\", \"ResNet50\", \"MobileNetV2\", \"EfficientNetB0\"]\n",
    "image_sizes = [128, 256, 512]\n",
    "base_path = \"/kaggle/input/traffic-prediction-models-plotting-value/traffic_prediction_models_Plotting_Value\"\n",
    "test_dir = '/kaggle/input/dhaka-traffic-classification-4-levels/Dhaka City Traffic Classification Dataset - 4-Level Congestion Analysis/test'\n",
    "\n",
    "# Initialize the model loading functions\n",
    "def load_model_for_inference(model_name, size, num_classes):\n",
    "    if model_name == \"ResNet50\":\n",
    "        model = ResNet50(input_shape=(size, size, 3), include_top=False, weights='imagenet')\n",
    "        model = tf.keras.Sequential([model,\n",
    "                                     tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                                     tf.keras.layers.Dense(num_classes, activation='softmax')])\n",
    "    elif model_name == \"MobileNetV2\":\n",
    "        model = MobileNetV2(input_shape=(size, size, 3), include_top=False, weights='imagenet')\n",
    "        model = tf.keras.Sequential([model,\n",
    "                                     tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                                     tf.keras.layers.Dense(num_classes, activation='softmax')])\n",
    "    elif model_name == \"EfficientNetB0\":\n",
    "        model = EfficientNetB0(input_shape=(size, size, 3), include_top=False, weights='imagenet')\n",
    "        model = tf.keras.Sequential([model,\n",
    "                                     tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                                     tf.keras.layers.Dense(num_classes, activation='softmax')])\n",
    "    else:\n",
    "        # Custom CNN model, allow for different input sizes\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(size, size, 3)),  # Specify the input shape here\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(num_classes, activation='softmax')  # Adjust for number of classes\n",
    "        ])\n",
    "    return model\n",
    "\n",
    "# Load the actual test data from the directory\n",
    "def load_test_data(test_dir, img_size):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "label_map = {}\n",
    "\n",
    "    # Get class names from subfolders in the test directory\n",
    "class_names = sorted(os.listdir(test_dir))\n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        label_map[idx] = class_name  # Map class index to class name\n",
    "        class_folder = os.path.join(test_dir, class_name)\n",
    "\n",
    "        # For each image in the class folder\n",
    "            for img_file in os.listdir(class_folder):\n",
    "if img_file.endswith('.jpg') or img_file.endswith('.png'):\n",
    "                image_paths.append(os.path.join(class_folder, img_file))\n",
    "                labels.append(idx)  # Store the class index\n",
    "    \n",
    "    # Load and preprocess images\n",
    "    images = []\n",
    "    for img_path in image_paths:\n",
    "        img = image.load_img(img_path, target_size=(img_size, img_size))\n",
    "        img_array = image.img_to_array(img)\n",
    "                img_array = preprocess_input(img_array)  # Preprocess according to model requirements\n",
    "        images.append(img_array)\n",
    "    \n",
    "    # Convert lists to arrays\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "return images, labels, class_names\n",
    "\n",
    "# Directory to save the classification reports\n",
    "save_dir = '/kaggle/working/classification_reports'\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Iterate through models and image sizes, and save classification report to JSON\n",
    "for model_name in models:\n",
    "    for size in image_sizes:\n",
    "        file_path = f\"{base_path}/{model_name}_Results_{size}.json\"\n",
    "\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file_path):\n",
    "            # Load the saved JSON data\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # Extract class names (from JSON) and true test labels\n",
    "            class_names = data.get(\"class_names\", [])\n",
    "            num_classes = len(class_names)  # Adjust number of classes based on the JSON\n",
    "\n",
    "            # Load test data (images and labels)\n",
    "            X_test, y_test, label_map = load_test_data(test_dir, size)\n",
    "\n",
    "            # Load the model\n",
    "            model = load_model_for_inference(model_name, size, num_classes)\n",
    "            \n",
    "            # Make predictions on the test data\n",
    "            predictions = model.predict(X_test)\n",
    "            y_pred_classes = np.argmax(predictions, axis=1)\n",
    "            \n",
    "            # Generate classification report with zero_division set to 1\n",
    "            report = classification_report(y_test, y_pred_classes,                                          target_names=class_names, output_dict=True, zero_division=1)\n",
    "\n",
    "            # Save the report as JSON\n",
    "            report_file = os.path.join(save_dir, f\"{model_name}_classification_report_{size}.json\")\n",
    "            with open(report_file, 'w') as f:\n",
    "                json.dump(report, f, indent=4)\n",
    "\n",
    "            # Convert the report to a pandas DataFrame for better table visualization\n",
    "            report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "            # Print the classification report as a table\n",
    "            print(f\"Classification report for {model_name} with image size {size}:\")\n",
    "            print(report_df)\n",
    "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "        else:\n",
    "            print(f\"File {file_path} does not exist.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6426556,
     "sourceId": 10374906,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6434950,
     "sourceId": 10387070,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
